{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sticky-pricing",
   "metadata": {
    "id": "sticky-pricing"
   },
   "source": [
    "# Proximal Policy Optimization (PPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nkc0JIu96ocM",
   "metadata": {
    "id": "nkc0JIu96ocM"
   },
   "source": [
    "### Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "median-quest",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3032,
     "status": "ok",
     "timestamp": 1652336439555,
     "user": {
      "displayName": "‍박성현[ 대학원석·박사통합과정재학 / 인공지능학과 ]",
      "userId": "06801221058335649108"
     },
     "user_tz": -540
    },
    "id": "median-quest",
    "outputId": "11912af5-a9ed-404b-918d-2b653aa7af69"
   },
   "outputs": [],
   "source": [
    "import random,datetime,gym,os,time,psutil,cv2,scipy.signal\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from gym.spaces import Box, Discrete\n",
    "from matplotlib import animation\n",
    "from IPython.display import display, HTML\n",
    "from collections import deque,namedtuple\n",
    "%matplotlib inline\n",
    "gym.logger.set_level(40)\n",
    "print (\"gym version:[%s]\"%(gym.__version__))\n",
    "print (\"Pytorch:[%s]\"%(torch.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rural-session",
   "metadata": {
    "id": "rural-session"
   },
   "source": [
    "### Util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intermediate-basketball",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1652336457605,
     "user": {
      "displayName": "‍박성현[ 대학원석·박사통합과정재학 / 인공지능학과 ]",
      "userId": "06801221058335649108"
     },
     "user_tz": -540
    },
    "id": "intermediate-basketball",
    "outputId": "41c89a0b-ebb7-4d46-a8f5-914b3d8e805d"
   },
   "outputs": [],
   "source": [
    "def discount_cumsum(x, gamma):\n",
    "    \"\"\"\n",
    "    Compute the discounted cumulative sums of vectors.\n",
    "    input:\n",
    "        vector x: [x0,x1,x2,x3]\n",
    "    output:\n",
    "        [\n",
    "         x0 + (gamma)*x1 + (gamma^2)*x2 + (gamma^3)*x3,\n",
    "         x1 + (gamma)*x2 + (gamma^2)*x3,\n",
    "         x2 + (gamma)*x3\n",
    "         x3\n",
    "        ]\n",
    "    \"\"\"\n",
    "    return scipy.signal.lfilter([1], [1, float(-gamma)], x[::-1], axis=0)[::-1]\n",
    "\n",
    "def combined_shape(length, shape=None):\n",
    "    if shape is None:\n",
    "        return (length,)\n",
    "    return (length, shape) if np.isscalar(shape) else (length, *shape)\n",
    "\n",
    "def statistics_scalar(x, with_min_and_max=False):\n",
    "    \"\"\"\n",
    "    Get mean/std and optional min/max of scalar x\n",
    "    Args:\n",
    "        x: An array containing samples of the scalar to produce statistics for.\n",
    "        with_min_and_max (bool): If true, return min and max of x in\n",
    "            addition to mean and std.\n",
    "    \"\"\"\n",
    "    x = np.array(x, dtype=np.float32)\n",
    "    global_sum, global_n = np.sum(x), len(x)\n",
    "    mean = global_sum / global_n\n",
    "    global_sum_sq = np.sum((x - mean) ** 2)\n",
    "    std = np.sqrt(global_sum_sq / global_n)  # compute global std\n",
    "    if with_min_and_max:\n",
    "        global_min = (np.min(x) if len(x) > 0 else np.inf)\n",
    "        global_max = (np.max(x) if len(x) > 0 else -np.inf)\n",
    "        return mean, std, global_min, global_max\n",
    "    return mean, std\n",
    "\n",
    "class PPOBuffer:\n",
    "    \"\"\"\n",
    "    A buffer for storing trajectories experienced by a PPO agent interacting\n",
    "    with the environment, and using Generalized Advantage Estimation (GAE-Lambda)\n",
    "    for calculating the advantages of state-action pairs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, odim, adim, size=5000, gamma=0.99, lam=0.95):\n",
    "        self.obs_buf = np.zeros(combined_shape(size, odim), dtype=np.float32)\n",
    "        self.act_buf = np.zeros(combined_shape(size, adim), dtype=np.float32)\n",
    "        self.adv_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.rew_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.ret_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.val_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.logp_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.gamma, self.lam = gamma, lam\n",
    "        self.ptr, self.path_start_idx, self.max_size = 0, 0, size\n",
    "\n",
    "    def store(self, obs, act, rew, val, logp):\n",
    "        \"\"\"\n",
    "        Append one timestep of agent-environment interaction to the buffer.\n",
    "        \"\"\"\n",
    "        assert self.ptr < self.max_size  # buffer has to have room so you can store\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.act_buf[self.ptr] = act\n",
    "        self.rew_buf[self.ptr] = rew\n",
    "        self.val_buf[self.ptr] = val\n",
    "        self.logp_buf[self.ptr] = logp\n",
    "        self.ptr += 1\n",
    "\n",
    "    def finish_path(self, last_val=0):\n",
    "        \"\"\"\n",
    "        Call this at the end of a trajectory, or when one gets cut off\n",
    "        by an epoch ending. This looks back in the buffer to where the\n",
    "        trajectory started, and uses rewards and value estimates from\n",
    "        the whole trajectory to compute advantage estimates with GAE-Lambda,\n",
    "        as well as compute the rewards-to-go for each state, to use as\n",
    "        the targets for the value function.\n",
    "\n",
    "        The \"last_val\" argument should be 0 if the trajectory ended\n",
    "        because the agent reached a terminal state (died), and otherwise\n",
    "        should be V(s_T), the value function estimated for the last state.\n",
    "        This allows us to bootstrap the reward-to-go calculation to account\n",
    "        for timesteps beyond the arbitrary episode horizon (or epoch cutoff).\n",
    "        \"\"\"\n",
    "        path_slice = slice(self.path_start_idx, self.ptr)\n",
    "        rews = np.append(self.rew_buf[path_slice], last_val)\n",
    "        vals = np.append(self.val_buf[path_slice], last_val)\n",
    "        # the next two lines implement GAE-Lambda advantage calculation\n",
    "        deltas = rews[:-1] + self.gamma * vals[1:] - vals[:-1]\n",
    "        self.adv_buf[path_slice] = discount_cumsum(deltas, self.gamma * self.lam)\n",
    "        # the next line computes rewards-to-go, to be targets for the value function\n",
    "        self.ret_buf[path_slice] = discount_cumsum(rews, self.gamma)[:-1]\n",
    "        self.path_start_idx = self.ptr\n",
    "\n",
    "    def get(self):\n",
    "        \"\"\"\n",
    "        Call this at the end of an epoch to get all of the data from\n",
    "        the buffer, with advantages appropriately normalized (shifted to have\n",
    "        mean zero and std one). Also, resets some pointers in the buffer.\n",
    "        \"\"\"\n",
    "        assert self.ptr == self.max_size  # buffer has to be full before you can get\n",
    "        self.ptr, self.path_start_idx = 0, 0\n",
    "        # the next two lines implement the advantage normalization trick\n",
    "        adv_mean, adv_std = statistics_scalar(self.adv_buf)\n",
    "        self.adv_buf = (self.adv_buf - adv_mean) / adv_std\n",
    "        return [self.obs_buf, self.act_buf, self.adv_buf,\n",
    "                self.ret_buf, self.logp_buf]\n",
    "    \n",
    "# Animation function \n",
    "def display_animation(anim):\n",
    "    plt.close(anim._fig)\n",
    "    return HTML(anim.to_jshtml())\n",
    "\n",
    "def display_frames_as_gif(frames):\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "    anim = animation.FuncAnimation(\n",
    "        plt.gcf(),animate,frames=len(frames),interval=10)\n",
    "    display(display_animation(anim))\n",
    "    \n",
    "print (\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governmental-marsh",
   "metadata": {
    "id": "governmental-marsh"
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sought-advocacy",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1652336457605,
     "user": {
      "displayName": "‍박성현[ 대학원석·박사통합과정재학 / 인공지능학과 ]",
      "userId": "06801221058335649108"
     },
     "user_tz": -540
    },
    "id": "sought-advocacy",
    "outputId": "0bd4e8e4-a69b-40c7-ad18-320c49aeaa3c"
   },
   "outputs": [],
   "source": [
    "def mlp(odim=24, hdims=[256,256]):\n",
    "    layers = []\n",
    "    layers.append(nn.Linear(odim,hdims[0]))\n",
    "    layers.append(nn.ReLU())\n",
    "    for idx, hdim in enumerate(hdims):\n",
    "        if idx < len(hdims)-2:\n",
    "            layers.append(nn.Linear(hdim,hdims[idx+1]))\n",
    "            layers.append(nn.ReLU())\n",
    "        elif idx == len(hdims)-1 :\n",
    "            layers.append(nn.Linear(hdims[idx-1],hdim))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def gaussian_likelihood(x, mu, log_std):\n",
    "    EPS = 1e-8\n",
    "    pre_sum = -0.5 * (((x-mu)/(torch.exp(log_std)+EPS))**2 + 2*log_std + np.log(2*np.pi))\n",
    "    return torch.sum(pre_sum, axis=1)\n",
    "\n",
    "class CategoricalPolicy(nn.Module):\n",
    "    def __init__(self, odim, adim, hdims=[64,64]):\n",
    "        super(CategoricalPolicy, self).__init__()\n",
    "        self.net = mlp(odim, hdims=hdims)\n",
    "        self.logits = nn.Linear(hdims[-1],adim)\n",
    "    def forward(self, x, a=None):\n",
    "        output = self.net(x)\n",
    "        logits = self.logits(output)\n",
    "        prob = F.softmax(logits)\n",
    "        dist = torch.distributions.Categorical(probs=prob)\n",
    "        pi = dist.sample()\n",
    "        logp_pi = dist.log_prob(pi)\n",
    "        logp = dist.log_prob(a)\n",
    "        return pi, logp, logp_pi, pi\n",
    "\n",
    "class GaussianPolicy(nn.Module):    \n",
    "    def __init__(self, odim, adim, hdims=[64,64], actv='relu', output_actv=None):\n",
    "        super(GaussianPolicy, self).__init__()\n",
    "        self.mu = mlp(odim, hdims=hdims+[adim])\n",
    "        self.log_std = 0.5 * torch.ones(adim)\n",
    "    def forward(self, x, a=None):\n",
    "        mu = self.mu(x)\n",
    "        log_std = self.log_std\n",
    "        std = torch.exp(log_std)\n",
    "        policy = torch.distributions.Normal(mu, std)\n",
    "        pi = policy.sample()\n",
    "        logp_pi = gaussian_likelihood(pi, mu, log_std)\n",
    "        if a is not None:\n",
    "            logp = gaussian_likelihood(a, mu, log_std)\n",
    "        else:\n",
    "            logp = None\n",
    "        return pi, logp, logp_pi, mu        \n",
    "\n",
    "class ActorCritic(nn.Module):   # def mlp_actor_critic\n",
    "    def __init__(self, odim, adim, hdims=[64,64],policy=None, action_space=None):\n",
    "        super(ActorCritic,self).__init__()\n",
    "        if (policy is None) and isinstance(action_space, Box):\n",
    "            # Gaussian policy for continuous actions\n",
    "            self.policy = GaussianPolicy(odim, adim, hdims)\n",
    "        elif (policy is None) and isinstance(action_space, Discrete):\n",
    "            # Categorical policy for discrete actions\n",
    "            self.policy = CategoricalPolicy(odim, adim, hdims)\n",
    "        self.vf_mlp = mlp(odim, hdims=hdims+[1])\n",
    "    def forward(self, x, a=None):\n",
    "        pi, logp, logp_pi, mu = self.policy(x, a)\n",
    "        v = self.vf_mlp(x)\n",
    "        return pi, logp, logp_pi, v, mu\n",
    "print (\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prepared-boring",
   "metadata": {
    "id": "prepared-boring"
   },
   "source": [
    "### PPO Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bibliographic-reply",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1652336457606,
     "user": {
      "displayName": "‍박성현[ 대학원석·박사통합과정재학 / 인공지능학과 ]",
      "userId": "06801221058335649108"
     },
     "user_tz": -540
    },
    "id": "bibliographic-reply",
    "outputId": "6eaf5197-16fd-4e89-c0d0-4f0c84b7a4f3"
   },
   "outputs": [],
   "source": [
    "def get_envs():\n",
    "    \"\"\"\n",
    "    Get environments\n",
    "    \"\"\"\n",
    "    env = gym.make('Ant-v2',render_mode='rgb_array')\n",
    "    # Reset env for evaluation\n",
    "    eval_env = gym.make('Ant-v2',render_mode='rgb_array')\n",
    "    _,_ = eval_env.reset()\n",
    "    for _ in range(3): # dummy run for proper rendering\n",
    "        a = eval_env.action_space.sample()\n",
    "        o,r,d,_,_ = eval_env.step(a)\n",
    "        time.sleep(0.01)\n",
    "    return env,eval_env\n",
    "\n",
    "class Agent(object):\n",
    "    def __init__(self,hdims=[256,256],\n",
    "                 clip_ratio=0.2,target_kl=0.01,\n",
    "                 gamma=0.99,lam=0.95,\n",
    "                 pi_lr=3e-4,vf_lr=1e-3,epsilon=1e-2,\n",
    "                 train_pi_iters=100,train_v_iters=100,\n",
    "                 epochs=1000,max_ep_len=1000,steps_per_epoch=5000,ep_len_rollout=500,batch_size=4096,\n",
    "                 print_every=10,evaluate_every=50,update_every=10\n",
    "                 ):\n",
    "        \n",
    "        # Model\n",
    "        self.hdims           = hdims\n",
    "        \n",
    "        # PPO hyperparameters\n",
    "        self.clip_ratio      = clip_ratio       # clip ratio of PPO\n",
    "        self.target_kl       = target_kl        # target KL divergence\n",
    "        \n",
    "        # PPO Buffer (GAE)\n",
    "        self.gamma           = gamma            # discount factor\n",
    "        self.lam             = lam              # GAE lambda\n",
    "        \n",
    "        # Update hyperparameters\n",
    "        self.pi_lr           = pi_lr            # policy learning rate\n",
    "        self.vf_lr           = vf_lr            # value learning rate\n",
    "        self.epsilon         = epsilon          # epsilon of adam\n",
    "        \n",
    "        # Update\n",
    "        self.train_pi_iters  = train_pi_iters   # policy update iterations\n",
    "        self.train_v_iters   = train_v_iters    # value update iterations\n",
    "        self.epochs          = epochs\n",
    "        self.max_ep_len      = max_ep_len\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        self.ep_len_rollout  = ep_len_rollout\n",
    "        self.batch_size      = batch_size\n",
    "        \n",
    "        self.print_every     = print_every\n",
    "        self.evaluate_every  = evaluate_every\n",
    "        self.update_every    = update_every\n",
    "        \n",
    "        # Environment\n",
    "        self.env, self.eval_env = get_envs()\n",
    "        odim = self.env.observation_space.shape[0]\n",
    "        adim = self.env.action_space.shape[0]\n",
    "\n",
    "        # Actor-critic model\n",
    "        self.actor_critic = ActorCritic(odim,adim,hdims=self.hdims,\n",
    "                                        action_space=self.env.action_space)\n",
    "        self.buf = PPOBuffer(odim=odim,adim=adim,\n",
    "                             size=self.steps_per_epoch, gamma=self.gamma,lam=self.lam)\n",
    "\n",
    "        # Optimizers\n",
    "        self.train_pi = optim.Adam(self.actor_critic.policy.parameters(),lr=self.pi_lr)\n",
    "        self.train_v = optim.Adam(self.actor_critic.vf_mlp.parameters(),lr=self.vf_lr)\n",
    "\n",
    "    def update_ppo(self, obs, act, adv, ret, logp):\n",
    "        logp_a_old = logp\n",
    "        pi_loss, v_loss = 0., 0.\n",
    "\n",
    "        # Update policy\n",
    "        for _ in range(self.train_pi_iters):\n",
    "            _, logp_a, _, _ = self.actor_critic.policy(obs, act)\n",
    "            ratio = torch.exp(logp_a - logp_a_old)  # pi(a|s) / pi_old(a|s)\n",
    "            min_adv = torch.where(adv > 0, (1 + self.clip_ratio) * adv, (1 - self.clip_ratio) * adv)\n",
    "            pi_loss = -torch.mean(torch.minimum(ratio * adv, min_adv))\n",
    "                \n",
    "            # Gradient clipping \n",
    "            self.train_pi.zero_grad()\n",
    "            pi_loss.backward()\n",
    "            self.train_pi.step()\n",
    "\n",
    "            # KL divergence upper-bound (trust region)\n",
    "            kl = torch.mean(logp_a_old - logp_a)\n",
    "            if kl > 1.5 * self.target_kl:\n",
    "                break\n",
    "\n",
    "        # Update value\n",
    "        for _ in range(self.train_v_iters):\n",
    "            v = torch.squeeze(self.actor_critic.vf_mlp(obs))\n",
    "            v_loss = F.mse_loss(v, ret)\n",
    "\n",
    "            self.train_v.zero_grad()\n",
    "            v_loss.backward()   \n",
    "            self.train_v.step()\n",
    "            \n",
    "        return pi_loss, v_loss\n",
    "\n",
    "    def train(self):\n",
    "        start_time = time.time()\n",
    "        o,_ = self.eval_env.reset()\n",
    "        r, d, ep_ret, ep_len, n_env_step = 0, False, 0, 0, 0\n",
    "        for epoch in range(self.epochs):\n",
    "            o,_ = self.env.reset()\n",
    "            for t in range(self.steps_per_epoch):\n",
    "                a, _, logp_t, v_t, _ = self.actor_critic(torch.FloatTensor(o.reshape(1, -1)))\n",
    "                o2,r,d,_,_ = self.env.step(a.numpy()[0])\n",
    "                if r < 0.0: r = 0.0\n",
    "                r = r + 0.01\n",
    "                ep_ret += r\n",
    "                ep_len += 1\n",
    "                n_env_step += 1\n",
    "\n",
    "                # Save the Experience to our buffer\n",
    "                self.buf.store(o, a, r, v_t, logp_t)\n",
    "                o = o2\n",
    "\n",
    "                terminal = d or (ep_len == self.max_ep_len)\n",
    "                if terminal or (t == (self.steps_per_epoch - 1)):\n",
    "                    # if trajectory didn't reach terminal state, bootstrap value target\n",
    "                    last_val = 0 if d else self.actor_critic.vf_mlp(torch.FloatTensor(o.reshape(1, -1))).detach().numpy()[0][0]\n",
    "                    self.buf.finish_path(last_val)\n",
    "                    ep_ret_save = np.copy(ep_ret)\n",
    "                    o,_ = self.env.reset()\n",
    "                    ep_ret, ep_len = 0, 0\n",
    "\n",
    "            # PPO update\n",
    "            obs, act, adv, ret, logp = [torch.tensor(x) for x in self.buf.get()]\n",
    "            self.update_ppo(obs, act, adv, ret, logp)\n",
    "            \n",
    "            # Print \n",
    "            if (epoch == 0) or (((epoch + 1) % self.print_every) == 0):\n",
    "                print(\"[%d/%d] ep_ret_save:[%.3f]\" % (epoch+1,self.epochs,ep_ret_save))\n",
    "\n",
    "            # Evaluate \n",
    "            if (epoch == 0) or (((epoch + 1) % self.evaluate_every) == 0):\n",
    "                ram_percent = psutil.virtual_memory().percent  # memory usage\n",
    "                print(\"[Eval. start] step:[%d/%d][%.1f%%] #step:[%.1e] time:[%s] ram:[%.1f%%].\" %\n",
    "                      (epoch + 1, self.epochs, epoch / self.epochs * 100,\n",
    "                       n_env_step,\n",
    "                       time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - start_time)),\n",
    "                       ram_percent)\n",
    "                      )\n",
    "                o,_ = self.eval_env.reset()\n",
    "                d, ep_ret, ep_len = False, 0, 0\n",
    "                frames = []\n",
    "                while not (d or (ep_len == self.max_ep_len)):\n",
    "                    a, _, _, _ = self.actor_critic.policy(torch.FloatTensor(o.reshape(1, -1)))\n",
    "                    o,r,d,_,_ = self.eval_env.step(a.numpy()[0])\n",
    "                    if r < 0.0: r = 0.0\n",
    "                    r = r + 0.01\n",
    "                    frame = self.eval_env.render()\n",
    "                    texted_frame = cv2.putText(\n",
    "                        img=np.copy(frame),\n",
    "                        text='tick:[%d]'%(ep_len),\n",
    "                        org=(80,30),fontFace=2,fontScale=0.8,color=(0,0,255),thickness=1)\n",
    "                    if (ep_len%5) == 0:\n",
    "                        frames.append(texted_frame)\n",
    "                    ep_ret += r  # compute return\n",
    "                    ep_len += 1\n",
    "                display_frames_as_gif(frames)\n",
    "                print(\"[Eval. done] episode return:[%.4f] length:[%d]\" % (ep_ret, ep_len))\n",
    "                \n",
    "print (\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "voluntary-scanning",
   "metadata": {
    "id": "voluntary-scanning"
   },
   "source": [
    "### Train an Ant agent with PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gothic-edmonton",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1X-Vb0dFljyV9NjiVPTxhuLInzA3e47m5"
    },
    "executionInfo": {
     "elapsed": 2726708,
     "status": "ok",
     "timestamp": 1652339184301,
     "user": {
      "displayName": "‍박성현[ 대학원석·박사통합과정재학 / 인공지능학과 ]",
      "userId": "06801221058335649108"
     },
     "user_tz": -540
    },
    "id": "gothic-edmonton",
    "outputId": "01c55658-fb02-4abf-9a0b-3ebc4ac6922f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "A = Agent(hdims=[256,256],\n",
    "          clip_ratio=0.5,target_kl=0.01,\n",
    "          gamma=0.99,lam=0.95,\n",
    "          pi_lr=1e-4,vf_lr=1e-4,epsilon=1e-2,\n",
    "          train_pi_iters=100,train_v_iters=100,\n",
    "          epochs=10000,max_ep_len=500,steps_per_epoch=5000,ep_len_rollout=500,batch_size=4096,\n",
    "          print_every=20,evaluate_every=50,update_every=10)\n",
    "A.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962c4d1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "09_ppo_pytorch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
