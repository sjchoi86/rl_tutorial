{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "confirmed-custom",
   "metadata": {
    "id": "confirmed-custom"
   },
   "source": [
    "# Soft Actor Critic (SAC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "O3Iq_iYpDRdN",
   "metadata": {
    "id": "O3Iq_iYpDRdN"
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "religious-viking",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4705,
     "status": "ok",
     "timestamp": 1652355619157,
     "user": {
      "displayName": "‍박성현[ 대학원석·박사통합과정재학 / 인공지능학과 ]",
      "userId": "06801221058335649108"
     },
     "user_tz": -540
    },
    "id": "religious-viking",
    "outputId": "9d70764c-f3e1-4233-8b0e-4e0fe553c5fa"
   },
   "outputs": [],
   "source": [
    "import random,datetime,gym,os,time,psutil,cv2,scipy.signal\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from gym.spaces import Box, Discrete\n",
    "from matplotlib import animation\n",
    "from IPython.display import display, HTML\n",
    "from collections import deque,namedtuple\n",
    "%matplotlib inline\n",
    "gym.logger.set_level(40)\n",
    "print (\"gym version:[%s]\"%(gym.__version__))\n",
    "print (\"Pytorch:[%s]\"%(torch.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "homeless-invitation",
   "metadata": {
    "id": "homeless-invitation"
   },
   "source": [
    "### Util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-english",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1652355619158,
     "user": {
      "displayName": "‍박성현[ 대학원석·박사통합과정재학 / 인공지능학과 ]",
      "userId": "06801221058335649108"
     },
     "user_tz": -540
    },
    "id": "offensive-english",
    "outputId": "c393f4f8-d30f-46ee-fbfe-1be14b9a8dcf"
   },
   "outputs": [],
   "source": [
    "def combined_shape(length, shape=None):\n",
    "    if shape is None:\n",
    "        return (length,)\n",
    "    return (length, shape) if np.isscalar(shape) else (length, *shape)\n",
    "\n",
    "class SACBuffer:\n",
    "    \"\"\"\n",
    "    A buffer for storing trajectories experienced by a SAC agent interacting\n",
    "    with the environment, and using Generalized Advantage Estimation (GAE-Lambda)\n",
    "    for calculating the advantages of state-action pairs.\n",
    "    \"\"\"\n",
    "    def __init__(self, odim, adim, size=5000):\n",
    "        self.obs1_buf = np.zeros(combined_shape(size, odim), dtype=np.float32)\n",
    "        self.obs2_buf = np.zeros(combined_shape(size, odim), dtype=np.float32)\n",
    "        self.acts_buf = np.zeros(combined_shape(size, adim), dtype=np.float32)\n",
    "        self.rews_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.done_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.ptr, self.size, self.max_size = 0, 0, size\n",
    "\n",
    "    def store(self, obs, act, rew, next_obs, done):\n",
    "        \"\"\"\n",
    "        Append one timestep of agent-environment interaction to the buffer.\n",
    "        \"\"\"\n",
    "        assert self.ptr < self.max_size  # buffer has to have room so you can store\n",
    "        self.obs1_buf[self.ptr] = obs\n",
    "        self.obs2_buf[self.ptr] = next_obs\n",
    "        self.acts_buf[self.ptr] = act\n",
    "        self.rews_buf[self.ptr] = rew\n",
    "        self.done_buf[self.ptr] = done\n",
    "        self.ptr = (self.ptr+1) % self.max_size\n",
    "        self.size = min(self.size+1, self.max_size)\n",
    "\n",
    "    def sample_batch(self, batch_size=32):\n",
    "        idxs = np.random.randint(0, self.size, size=batch_size)\n",
    "        batch = dict(obs1=self.obs1_buf[idxs],\n",
    "                     obs2=self.obs2_buf[idxs],\n",
    "                     acts=self.acts_buf[idxs],\n",
    "                     rews=self.rews_buf[idxs],\n",
    "                     done=self.done_buf[idxs])\n",
    "        return {k: v for k, v in batch.items()}\n",
    "\n",
    "    def get(self):\n",
    "        names = ['obs1_buf','obs2_buf','acts_buf','rews_buf','done_buf',\n",
    "                 'ptr','size','max_size']\n",
    "        vals =[self.obs1_buf,self.obs2_buf,self.acts_buf,self.rews_buf,self.done_buf,\n",
    "               self.ptr,self.size,self.max_size]\n",
    "        return names,vals\n",
    "\n",
    "    def restore(self,a):\n",
    "        self.obs1_buf = a[0]\n",
    "        self.obs2_buf = a[1]\n",
    "        self.acts_buf = a[2]\n",
    "        self.rews_buf = a[3]\n",
    "        self.done_buf = a[4]\n",
    "        self.ptr = a[5]\n",
    "        self.size = a[6]\n",
    "        self.max_size = a[7]\n",
    "\n",
    "def display_animation(anim):\n",
    "    plt.close(anim._fig)\n",
    "    return HTML(anim.to_jshtml())\n",
    "\n",
    "def display_frames_as_gif(frames):\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "    anim = animation.FuncAnimation(\n",
    "        plt.gcf(),animate,frames=len(frames),interval=10)\n",
    "    display(display_animation(anim))\n",
    "    \n",
    "print (\"Done.\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decimal-premium",
   "metadata": {
    "id": "decimal-premium"
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arabic-intention",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 813,
     "status": "ok",
     "timestamp": 1652355619963,
     "user": {
      "displayName": "‍박성현[ 대학원석·박사통합과정재학 / 인공지능학과 ]",
      "userId": "06801221058335649108"
     },
     "user_tz": -540
    },
    "id": "arabic-intention",
    "outputId": "0e85d100-0438-4946-cb3e-33d07f67893d"
   },
   "outputs": [],
   "source": [
    "def mlp(odim=24, hdims=[256, 256]):\n",
    "    layers = []\n",
    "    layers.append(nn.Linear(odim,hdims[0]))\n",
    "    layers.append(nn.ReLU())\n",
    "    for idx, hdim in enumerate(hdims):\n",
    "        if idx < len(hdims)-2:\n",
    "            layers.append(nn.Linear(hdim,hdims[idx+1]))\n",
    "            layers.append(nn.ReLU())\n",
    "        elif idx == len(hdims)-1 :\n",
    "            layers.append(nn.Linear(hdims[idx-1],hdim))\n",
    "\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def gaussian_loglik(x,mu,log_std):\n",
    "    EPS = 1e-8\n",
    "    pre_sum = -0.5*(( (x-mu)/(torch.exp(log_std)+EPS) )**2 + 2*log_std + np.log(2*np.pi))\n",
    "    return torch.sum(pre_sum, axis=1)\n",
    "\n",
    "class GaussianPolicy(nn.Module):\n",
    "    def __init__(self,odim,adim,hdims=[256,256]):\n",
    "        super(GaussianPolicy, self).__init__()\n",
    "        self.odim    = odim\n",
    "        self.adim    = adim\n",
    "        self.hdims   = hdims\n",
    "        # Define network\n",
    "        self.net     = mlp(self.odim,self.hdims)\n",
    "        self.mu      = nn.Linear(self.hdims[-1],self.adim)\n",
    "        self.log_std = nn.Linear(self.hdims[-1],self.adim)\n",
    "\n",
    "    def forward(self, o, get_logprob=True):\n",
    "        net_ouput = self.net(o)\n",
    "        mu = self.mu(net_ouput)\n",
    "        log_std = self.log_std(net_ouput)\n",
    "\n",
    "        LOG_STD_MIN, LOG_STD_MAX = -10.0, +2.0\n",
    "        log_std = torch.clip(log_std, LOG_STD_MIN, LOG_STD_MAX) #log_std\n",
    "        std = torch.exp(log_std) \n",
    "        dist = torch.distributions.Normal(mu, std)\n",
    "        pi = dist.sample()   \n",
    "\n",
    "        if get_logprob:\n",
    "            # Compute logprob from Gaussian, and then apply correction for Tanh squashing.\n",
    "            # NOTE: The correction formula is a little bit magic. To get an understanding\n",
    "            # of where it comes from, check out the original SAC paper (arXiv 1801.01290)\n",
    "            # and look in appendix C. This is a more numerically-stable equivalent to Eq 21.\n",
    "            # Try deriving it yourself as a (very difficult) exercise. :)\n",
    "            logp_pi = gaussian_loglik(x=pi, mu=mu, log_std=log_std)\n",
    "            logp_pi -= torch.sum(2*(np.log(2) - pi - F.softplus(-2*pi)), axis=1)\n",
    "        else:\n",
    "            logp_pi = None\n",
    "        mu, pi = torch.tanh(mu), torch.tanh(pi) # squach action \n",
    "\n",
    "        return mu, pi, logp_pi\n",
    "\n",
    "class QFunction(nn.Module):\n",
    "    def __init__(self,odim,adim,hdims=[256,256]):\n",
    "        super(QFunction, self).__init__()\n",
    "        self.q = mlp(odim+adim, hdims=hdims)\n",
    "        self.q2 = nn.Linear(hdims[-1],1)\n",
    "\n",
    "    def forward(self, o, a):\n",
    "        x = torch.concat([o, a], -1)\n",
    "        q = self.q(x)\n",
    "        q = self.q2(q)\n",
    "        return torch.squeeze(q, axis=1)\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self,odim,adim,hdims=[256,256],\n",
    "                 alpha_pi=0.1,alpha_q=0.1,gamma=0.98,lr=3e-4):\n",
    "        super(ActorCritic,self).__init__()\n",
    "        self.odim       = odim\n",
    "        self.adim       = adim\n",
    "        self.hdims      = hdims\n",
    "        \n",
    "        self.alpha_pi   = alpha_pi\n",
    "        self.alpha_q    = alpha_q\n",
    "        self.gamma      = gamma\n",
    "        self.lr         = lr\n",
    "        \n",
    "        # Define policy and value functions\n",
    "        self.policy = GaussianPolicy(\n",
    "            odim=self.odim,adim=self.adim,hdims=self.hdims)\n",
    "        self.q1 = QFunction(\n",
    "            odim=self.odim,adim=self.adim,hdims=self.hdims)\n",
    "        self.q2 = QFunction(\n",
    "            odim=self.odim,adim=self.adim,hdims=self.hdims)\n",
    "        \n",
    "        # Optimizers\n",
    "        self.train_pi = optim.Adam(self.policy.parameters(),lr=self.lr)\n",
    "        self.train_q1 = optim.Adam(self.q1.parameters(), lr=self.lr)\n",
    "        self.train_q2 = optim.Adam(self.q2.parameters(), lr=self.lr)\n",
    "\n",
    "    def forward(self, o, deterministic=False):\n",
    "        mu, pi, _ = self.policy(o, False)\n",
    "        if deterministic: return mu\n",
    "        else: return pi\n",
    "\n",
    "    def update_policy(self, data):\n",
    "        o = data['obs1']\n",
    "        _, pi, logp_pi = self.policy(o)\n",
    "        q1_pi = self.q1(o, pi)\n",
    "        q2_pi = self.q2(o, pi)\n",
    "        min_q_pi = torch.minimum(q1_pi, q2_pi)\n",
    "        pi_loss = torch.mean(self.alpha_pi*logp_pi - min_q_pi)\n",
    "\n",
    "        self.train_pi.zero_grad()\n",
    "        pi_loss.backward()\n",
    "        self.train_pi.step()\n",
    "\n",
    "        return pi_loss, logp_pi, min_q_pi\n",
    "\n",
    "    def update_Q(self, target, data):\n",
    "        o,a,r,o2,d = data['obs1'],data['acts'],data['rews'],data['obs2'],data['done']\n",
    "        _, pi_next, logp_pi_next = self.policy(o2)\n",
    "        q1_targ = target.q1(o2, pi_next)\n",
    "        q2_targ = target.q2(o2, pi_next)\n",
    "        min_q_targ = torch.minimum(q1_targ, q2_targ)\n",
    "        q_backup =  r + self.gamma*(1-d)*(min_q_targ - self.alpha_q*logp_pi_next)\n",
    "        q1 = self.q1(o, a)\n",
    "        q2 = self.q2(o, a)\n",
    "        q1_loss = 0.5*F.mse_loss(q1,q_backup.detach())\n",
    "        q2_loss = 0.5*F.mse_loss(q2,q_backup.detach())\n",
    "        value_loss = q1_loss + q2_loss\n",
    "\n",
    "        self.train_q1.zero_grad()\n",
    "        q1_loss.backward()\n",
    "        self.train_q1.step()\n",
    "\n",
    "        self.train_q2.zero_grad()\n",
    "        q2_loss.backward()\n",
    "        self.train_q2.step()\n",
    "\n",
    "        return value_loss, q1, q2, logp_pi_next, q_backup, q1_targ, q2_targ\n",
    "\n",
    "print (\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "religious-training",
   "metadata": {
    "id": "religious-training"
   },
   "source": [
    "### SAC Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recognized-collins",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 437,
     "status": "ok",
     "timestamp": 1652355620394,
     "user": {
      "displayName": "‍박성현[ 대학원석·박사통합과정재학 / 인공지능학과 ]",
      "userId": "06801221058335649108"
     },
     "user_tz": -540
    },
    "id": "recognized-collins",
    "outputId": "f8f62a6b-0658-42d2-9d1e-c6d6f7ee66ca"
   },
   "outputs": [],
   "source": [
    "def get_envs():\n",
    "    env = gym.make('Ant-v2',render_mode='rgb_array')\n",
    "    eval_env = gym.make('Ant-v2',render_mode='rgb_array')\n",
    "    _,_ = eval_env.reset()\n",
    "    for _ in range(3): # dummy run for proper rendering\n",
    "        a = eval_env.action_space.sample()\n",
    "        o,r,d,_,_ = eval_env.step(a)\n",
    "        time.sleep(0.01)\n",
    "    return env,eval_env\n",
    "\n",
    "class Agent(object):\n",
    "    def __init__(self,hdims=[256,256],alpha_pi=0.1,alpha_q=0.1,gamma=0.98,polyak=0.995,\n",
    "                 lr=3e-4,seed=1,\n",
    "                 buffer_size_short=1e5,buffer_size_long=1e6):\n",
    "        \"\"\"\n",
    "        Initialize SAC agent\n",
    "        \"\"\"\n",
    "        self.hdims              = hdims\n",
    "        self.alpha_pi           = alpha_pi\n",
    "        self.alpha_q            = alpha_q\n",
    "        self.gamma              = gamma\n",
    "        self.polyak             = polyak\n",
    "        \n",
    "        self.lr                 = lr\n",
    "        self.seed               = seed\n",
    "        \n",
    "        self.buffer_size_short  = buffer_size_short\n",
    "        self.buffer_size_long   = buffer_size_long\n",
    "        \n",
    "        # Environment\n",
    "        self.env, self.eval_env = get_envs()\n",
    "        odim, adim    = self.env.observation_space.shape[0],self.env.action_space.shape[0]\n",
    "        self.odim     = odim\n",
    "        self.adim     = adim\n",
    "\n",
    "        # Actor-critic model\n",
    "        torch.manual_seed(self.seed)\n",
    "        np.random.seed(self.seed)\n",
    "        random.seed(self.seed)\n",
    "        self.model = ActorCritic(self.odim,self.adim,hdims=self.hdims,\n",
    "                                 alpha_pi=self.alpha_pi,alpha_q=self.alpha_q,gamma=self.gamma,\n",
    "                                 lr=self.lr)\n",
    "        self.target = ActorCritic(self.odim,self.adim,hdims=self.hdims,\n",
    "                                  alpha_pi=self.alpha_pi,alpha_q=self.alpha_q,gamma=self.gamma,\n",
    "                                  lr=self.lr)\n",
    "        self.target.load_state_dict(self.model.state_dict())\n",
    "        # Buffers\n",
    "        self.replay_buffer_long = SACBuffer(odim=self.odim,adim=self.adim,\n",
    "                                            size=int(self.buffer_size_long))\n",
    "        self.replay_buffer_short = SACBuffer(odim=self.odim,adim=self.adim,\n",
    "                                             size=int(self.buffer_size_short))\n",
    "\n",
    "    def get_action(self, o, deterministic=False):\n",
    "        return self.model(torch.FloatTensor(o.reshape(1,-1)),deterministic)\n",
    "\n",
    "    def get_weights(self):\n",
    "        weight_vals = self.model.state_dict()\n",
    "        return weight_vals\n",
    "\n",
    "    def set_weights(self, weight_vals):\n",
    "        return self.model.load_state_dict(weight_vals)\n",
    "        \n",
    "    def update_sac(self, replay_buffer):\n",
    "        pi_loss, logp_pi, min_q_pi = self.model.update_policy(replay_buffer)\n",
    "        value_loss, q1, q2, logp_pi_next, q_backup, q1_targ, q2_targ = \\\n",
    "            self.model.update_Q(self.target, replay_buffer)\n",
    "\n",
    "        # Polyak averaging of value networks\n",
    "        for v_main, v_targ in zip(self.model.q1.parameters(),\n",
    "                                  self.target.q1.parameters()):\n",
    "            v_targ.data.copy_(v_main.data * (1 - self.polyak) + v_targ.data * self.polyak)\n",
    "        for v_main, v_targ in zip(self.model.q2.parameters(),\n",
    "                                  self.target.q2.parameters()):\n",
    "            v_targ.data.copy_(v_main.data * (1 - self.polyak) + v_targ.data * self.polyak)\n",
    "\n",
    "        return logp_pi, min_q_pi, logp_pi_next, q_backup, q1_targ, q2_targ\n",
    "\n",
    "    def train(self,total_steps=1e6,start_steps=1e4,evaluate_every=1e4,plot_every=1e4,\n",
    "              batch_size=128,update_count=2,max_ep_len_eval=1000,load_dir=None):\n",
    "        \"\"\"\n",
    "        Train SAC\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        [v_targ.data.copy_(v_main.data) for v_main, v_targ in zip(\n",
    "            self.model.parameters(), self.target.parameters()\n",
    "        )]\n",
    "        o,_ = self.env.reset()\n",
    "        r,d,ep_ret,ep_len,n_env_step = 0,False,0,0,0\n",
    "        for step in range(int(total_steps)):\n",
    "            # Step\n",
    "            if step > start_steps:\n",
    "                a = self.get_action(o, deterministic=False)\n",
    "                a = a.numpy()[0]\n",
    "            else:\n",
    "                a = self.env.action_space.sample()\n",
    "            o2,r,d,_,_ = self.env.step(a)\n",
    "            if r < 0.0: r = 0.0\n",
    "            r = r + 0.01\n",
    "            ep_len += 1\n",
    "            ep_ret += r\n",
    "\n",
    "            # Append\n",
    "            self.replay_buffer_long.store(o, a, r, o2, d)\n",
    "            self.replay_buffer_short.store(o, a, r, o2, d)\n",
    "            n_env_step += 1\n",
    "            o = o2\n",
    "\n",
    "            # Reset when done\n",
    "            if d:\n",
    "                o,_ = self.env.reset()\n",
    "                ep_ret, ep_len = 0, 0\n",
    "\n",
    "            # Update\n",
    "            if step >= start_steps:\n",
    "                for _ in range(update_count):\n",
    "                    batch = self.replay_buffer_long.sample_batch(batch_size//2)\n",
    "                    batch_short = self.replay_buffer_short.sample_batch(batch_size//2)\n",
    "                    batch = {k: torch.FloatTensor(v) for k, v in batch.items()}\n",
    "                    batch_short = {k: torch.FloatTensor(v) for k, v in batch_short.items()}\n",
    "                    replay_buffer = dict(obs1=torch.concat([batch['obs1'], batch_short['obs1']], 0),\n",
    "                                            obs2=torch.concat([batch['obs2'], batch_short['obs2']], 0),\n",
    "                                            acts=torch.concat([batch['acts'], batch_short['acts']], 0),\n",
    "                                            rews=torch.concat([batch['rews'], batch_short['rews']], 0),\n",
    "                                            done=torch.concat([batch['done'], batch_short['done']], 0))\n",
    "                    logp_pi, min_q_pi, logp_pi_next, q_backup, q1_targ, q2_targ = self.update_sac(replay_buffer)\n",
    "\n",
    "            # Evaluate\n",
    "            if (step==0) or (((step+1)%evaluate_every)==0) or (((step+1)%plot_every)==0):\n",
    "                ram_percent = psutil.virtual_memory().percent  # memory usage\n",
    "                print(\"[Eval. start] step:[%d/%d][%.1f%%] #step:[%.1e] time:[%s] ram:[%.1f%%].\" %\n",
    "                      (step + 1, total_steps, step / total_steps * 100,\n",
    "                       n_env_step,\n",
    "                       time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - start_time)),\n",
    "                       ram_percent)\n",
    "                      )\n",
    "                o,_ = self.eval_env.reset()\n",
    "                d, ep_ret, ep_len = False, 0, 0\n",
    "                _ = self.eval_env.render()\n",
    "                frames = []\n",
    "                while not (d or (ep_len == max_ep_len_eval)):\n",
    "                    a = self.get_action(o, deterministic=True)\n",
    "                    o,r,d,_,_ = self.eval_env.step(a.detach().numpy()[0])\n",
    "                    if r < 0.0: r = 0.0\n",
    "                    r = r + 0.01\n",
    "                    frame = self.eval_env.render()\n",
    "                    texted_frame = cv2.putText(\n",
    "                        img=np.copy(frame),\n",
    "                        text='tick:[%d]'%(ep_len),\n",
    "                        org=(80,30),fontFace=2,fontScale=0.8,color=(0,0,255),thickness=1)\n",
    "                    if (ep_len%5) == 0:\n",
    "                        frames.append(texted_frame)\n",
    "                    ep_ret += r  # compute return\n",
    "                    ep_len += 1\n",
    "                if (step==0) or (((step+1)%plot_every)==0):\n",
    "                    display_frames_as_gif(frames)\n",
    "                print(\"[Eval. done] ep_ret:[%.4f] ep_len:[%d]\"% (ep_ret, ep_len))\n",
    "    \n",
    "print (\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legitimate-increase",
   "metadata": {
    "id": "legitimate-increase"
   },
   "source": [
    "### Train an Ant agent with SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aboriginal-excellence",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 998,
     "output_embedded_package_id": "1b6TdyJ16e5rQAivqwQAGfv4dBTJEN-90"
    },
    "id": "aboriginal-excellence",
    "outputId": "bd945379-6bbe-4735-b89a-f34959095b1c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "A = Agent(hdims=[256,256],alpha_pi=0.1,alpha_q=0.1,gamma=0.98,polyak=0.995,\n",
    "          lr=1e-4,seed=1,buffer_size_short=5e3,buffer_size_long=1e5)\n",
    "A.train(total_steps=2e5,start_steps=1e4,evaluate_every=1e4,plot_every=5e4,\n",
    "        batch_size=128,update_count=2,max_ep_len_eval=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3275db5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "10_sac_pytorch.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "aeb032cadb1980e4ed464ef45c1fb61d136520dea1ead5a7148954a10232163c"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
